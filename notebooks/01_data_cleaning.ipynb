{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 ‚Äî Data Cleaning & Preparation\n",
        "\n",
        "> **Objective:** To load the raw public transit delay dataset, assess data quality, perform cleaning and feature engineering, and save a processed dataset for downstream exploratory analysis and modeling.\n",
        "\n",
        "This notebook outlines the following stages:\n",
        "1. [**Dataset overview**](#dataset-overview) ‚Äî loading raw data and inspecting structure  \n",
        "2. [**Missing values analysis**](#missing-values-analysis) ‚Äî assessing completeness and handling nulls  \n",
        "3. [**Data cleaning steps**](#data-cleaning-steps) ‚Äî addressing inconsistencies, types, and outliers  \n",
        "4. [**Feature engineering**](#feature-engineering) ‚Äî creating derived features for analysis  \n",
        "5. [**Save cleaned dataset**](#save-cleaned-dataset) ‚Äî exporting to `data/processed/`  \n",
        "\n",
        "> **Note:** Section links work in Jupyter or nbviewer; they may not render in static GitHub previews."
      ],
      "id": "title"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üß† Project Context\n",
        "\n",
        "This notebook is the first step in the **Public Transit Delay EDA** project. Clean, well-structured data is essential for reliable exploratory analysis and any subsequent modeling. All transformations applied here are documented so that the pipeline is reproducible."
      ],
      "id": "context"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üß∞ Imports <a id=\"imports\"></a>\n",
        "\n",
        "Core libraries for data loading, manipulation, and cleaning:\n",
        "\n",
        "- **pandas** ‚Äî data loading, tabular manipulation, and export  \n",
        "- **numpy** ‚Äî numerical operations where needed  \n",
        "- **pathlib / os** ‚Äî path handling for reading and writing files  "
      ],
      "id": "imports"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "imports-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üì• Dataset Overview <a id=\"dataset-overview\"></a>\n",
        "\n",
        "Load the raw dataset from `data/raw/` and inspect its structure: shape, column names, dtypes, and a sample of rows.  \n",
        "This confirms that the import completed successfully and provides a first look at the variables available for analysis."
      ],
      "id": "dataset-overview"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "raw_path = Path(\"../data/raw/public_transport_delays.csv\")\n",
        "df = pd.read_csv(raw_path)\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "load-data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Column | Description |\n",
        "|--------|-------------|\n",
        "| `trip_id` | Unique trip identifier |\n",
        "| `date` | Trip date |\n",
        "| `time` | Trip start time |\n",
        "| `transport_type` | Bus, Tram, Metro, or Train |\n",
        "| `route_id` | Route identifier (e.g. Route_1, Route_2) |\n",
        "| `origin_station`, `destination_station` | Start and end station IDs |\n",
        "| `scheduled_departure`, `scheduled_arrival` | Planned departure/arrival times |\n",
        "| `actual_departure_delay_min`, `actual_arrival_delay_min` | Delay in minutes (negative = early) |\n",
        "| `weather_condition` | Clear, Rain, Snow, Storm, Fog, Cloudy |\n",
        "| `temperature_C`, `humidity_percent`, `wind_speed_kmh`, `precipitation_mm` | Weather variables |\n",
        "| `event_type` | None, Sports, Concert, Parade, Protest, Festival |\n",
        "| `event_attendance_est` | Estimated event attendance |\n",
        "| `traffic_congestion_index` | Congestion level (0‚Äì100) |\n",
        "| `holiday` | 1 if holiday, 0 otherwise |\n",
        "| `peak_hour` | 1 if peak, 0 otherwise |\n",
        "| `weekday` | Day of week (0‚Äì6) in raw data |\n",
        "| `season` | Winter, Spring, Summer, Autumn |\n",
        "| `delayed` | 1 if trip was delayed (arrival delay > 0), 0 otherwise |"
      ],
      "id": "schema-table"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üßæ Missing Values Analysis <a id=\"missing-values-analysis\"></a>\n",
        "\n",
        "Summarize the dataset structure with `df.info()` and count nulls per column.  \n",
        "Identifying missing values is essential before cleaning so that imputation or removal strategies can be applied consistently."
      ],
      "id": "missing-header"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "info"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0e1650f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üîé *Summary*\n",
        "\n",
        "**Only `event_type` has missing values** (1,173 of 2,000 rows). No event was recorded for those trips. We will **fill these with the string `\"None\"`** so that EDA and modeling can treat \"no event\" as a distinct category. All other columns are complete."
      ],
      "id": "missing-insight"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üßπ Data Cleaning Steps <a id=\"data-cleaning-steps\"></a>\n",
        "\n",
        "Apply cleaning steps such as:\n",
        "- Correcting data types (dates, categories, numeric)  \n",
        "- Handling or imputing missing values  \n",
        "- Removing or flagging duplicates  \n",
        "- Addressing obvious outliers or invalid values  \n",
        "\n",
        "*(Replace the placeholder below with concrete cleaning code and brief comments.)*"
      ],
      "id": "cleaning-header"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parse date and build datetime for time-based features\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "df[\"datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"], errors=\"coerce\")\n",
        "\n",
        "# Fill missing event_type with \"None\" (no event)\n",
        "df[\"event_type\"] = df[\"event_type\"].fillna(\"None\")\n",
        "\n",
        "# Drop duplicate rows if any (by trip_id)\n",
        "n_before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"trip_id\"], keep=\"first\")\n",
        "print(f\"Dropped {n_before - len(df)} duplicate trip(s). Rows: {len(df)}\")\n",
        "\n",
        "# Ensure numeric delay columns are int (already are)\n",
        "df[[\"actual_departure_delay_min\", \"actual_arrival_delay_min\"]] = df[\n",
        "    [\"actual_departure_delay_min\", \"actual_arrival_delay_min\"]\n",
        "].astype(int)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cleaning-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### ‚öôÔ∏è Feature Engineering <a id=\"feature-engineering\"></a>\n",
        "\n",
        "Create derived features that may be useful for EDA and modeling, for example:\n",
        "- Time-based: hour of day, day of week, month, peak vs off-peak  \n",
        "- Delay-related: delay bins, on-time vs delayed flag  \n",
        "- Route or line aggregates  \n",
        "\n",
        "*(Replace the placeholder below with actual feature engineering code.)*"
      ],
      "id": "fe-header"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hour of day (0‚Äì23) and day of week (0=Monday, 6=Sunday)\n",
        "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "df[\"day_of_week\"] = df[\"datetime\"].dt.dayofweek\n",
        "\n",
        "# Primary delay for analysis: use arrival delay (passenger-facing)\n",
        "df[\"delay_minutes\"] = df[\"actual_arrival_delay_min\"].copy()\n",
        "\n",
        "# Delay category for interpretation\n",
        "def delay_category(minutes):\n",
        "    if minutes <= 0:\n",
        "        return \"On time\"\n",
        "    if minutes <= 5:\n",
        "        return \"Slight (1‚Äì5 min)\"\n",
        "    if minutes <= 15:\n",
        "        return \"Moderate (6‚Äì15 min)\"\n",
        "    return \"Severe (15+ min)\"\n",
        "\n",
        "df[\"delay_category\"] = df[\"delay_minutes\"].apply(delay_category)\n",
        "\n",
        "# Preview engineered columns\n",
        "df[[\"datetime\", \"hour\", \"day_of_week\", \"delay_minutes\", \"delay_category\"]].head(5)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fe-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### üíæ Save Cleaned Dataset <a id=\"save-cleaned-dataset\"></a>\n",
        "\n",
        "Export the cleaned and engineered dataset to `data/processed/` so that downstream notebooks (e.g. EDA) can load it without re-running cleaning steps."
      ],
      "id": "save-header"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "out_path = Path(\"../data/processed/transit_delays_cleaned.csv\")\n",
        "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(out_path, index=False)\n",
        "print(f\"Saved {len(df)} rows to {out_path}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "save-code"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}